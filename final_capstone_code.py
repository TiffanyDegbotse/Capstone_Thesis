# -*- coding: utf-8 -*-
"""Final_Capstone_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LRMWAleCfyV0Hw2M1WrE48XTtwgHplLI

# AI-Enhanced Video Editing: User-Driven Arbitrary Video-to-Video Style Transfer
This notebook implements the final system for applying the visual style of one video onto the content of another while preserving temporal consistency.

## 1. Installations and Dependencies
"""

!pip install tensorflow_hub

!pip install imageio[ffmpeg]

!pip install ffmpeg-python

!pip install torch torchvision opencv-python numpy matplotlib RAFT

!pip install torch torchvision numpy opencv-python tqdm Pillow

"""## 2. Imports"""

import ffmpeg

from google.colab import drive
drive.mount('/content/drive')

"""#Cloning RAFT"""

!git clone https://github.com/princeton-vl/RAFT.git
!cd RAFT

# Commented out IPython magic to ensure Python compatibility.
# %cd RAFT

!ls

!bash download_models.sh

!ls /content/RAFT

!grep -r "class RAFT" /content/RAFT/

!ls /content/RAFT/core

import sys
import os

# Add the RAFT core directory to the Python path
sys.path.append(os.path.abspath('/content/RAFT/core'))

!ls /content/RAFT/core/utils

from RAFT.core.raft import RAFT
from RAFT.core.utils.utils import InputPadder

"""# TensorFlow Version Fix. This pipeline requires TensorFlow 2.15.0 due to compatibility with certain dependencies (e.g., tensorflow_hub). If you're running this in Google Colab or another environment with a newer version of TensorFlow, please run this cell and restart the runtime when prompted."""

!pip uninstall tensorflow -y
!pip install tensorflow==2.15.0

"""#Check if correct version"""

import tensorflow as tf
print("TensorFlow version:", tf.__version__)

"""#Pipeline"""

import os
import cv2
import torch
import numpy as np
import shutil
import subprocess
from PIL import Image
import tensorflow as tf
import tensorflow_hub as hub
from argparse import Namespace
from collections import OrderedDict

# Load style transfer model
hub_model = hub.load("https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2")

# Initialize RAFT model
def load_raft_model(raft_weights_path):
    args = Namespace(small=False, mixed_precision=False, alternate_corr=False)
    model = RAFT(args)
    state_dict = torch.load(raft_weights_path, map_location="cuda")

# Remove 'module.' prefix
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        new_key = k.replace("module.", "") if k.startswith("module.") else k
        new_state_dict[new_key] = v

    model.load_state_dict(new_state_dict)
    model.cuda()
    model.eval()
    return model

# Denoising
def denoise_frame(frame):
    return cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)

#Stabilization
def stabilize_video(video_path, output_path):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))

    ret, prev = cap.read()
    if not ret:
        print("Failed to read first frame")
        return
    prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)

    orb = cv2.ORB_create(500)
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)

    while True:
        ret, curr = cap.read()
        if not ret:
            break
        curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)

        kp1, des1 = orb.detectAndCompute(prev_gray, None)
        kp2, des2 = orb.detectAndCompute(curr_gray, None)

        if des1 is not None and des2 is not None and len(kp1) >= 4 and len(kp2) >= 4:
            matches = bf.match(des1, des2)
            matches = sorted(matches, key=lambda x: x.distance)

            pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
            pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

            transform, _ = cv2.estimateAffine2D(pts1, pts2)
        else:
            transform = None

        if transform is not None:
            stabilized = cv2.warpAffine(curr, transform, (w, h))
        else:
            stabilized = curr

        out.write(stabilized)
        prev_gray = curr_gray

    cap.release()
    out.release()
    print("Stabilized video saved to:", output_path)

# Extract audio
def extract_audio(video_path, output_audio_path):
    subprocess.run([
        "ffmpeg", "-y", "-i", video_path,
        "-vn", "-acodec", "copy", output_audio_path
    ])

# Convert video to frames
def video_to_frames(video_path, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame = denoise_frame(frame)  # Apply denoising
        frame_path = os.path.join(output_dir, f"frame_{count:04d}.png")
        cv2.imwrite(frame_path, frame)
        count += 1
    cap.release()
    return count, fps

# Compile frames into video and merge audio
def frames_to_video(frame_dir, output_path, fps, audio_path):
    frame_files = sorted(
        [os.path.join(frame_dir, f) for f in os.listdir(frame_dir) if f.startswith("styled_frame_")]
    )
    if not frame_files:
        print("No styled frames to compile into video.")
        return

    first_frame = cv2.imread(frame_files[0])
    height, width, _ = first_frame.shape
    temp_video = output_path.replace(".mp4", "_no_audio.mp4")

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(temp_video, fourcc, fps, (width, height))

    for file in frame_files:
        frame = cv2.imread(file)
        out.write(frame)
    out.release()

    final_output = output_path
    subprocess.run([
        "ffmpeg", "-y", "-i", temp_video, "-i", audio_path,
        "-c:v", "copy", "-c:a", "aac", "-b:a", "192k", final_output
    ])
    os.remove(temp_video)

# Loads an image from a NumPy array and prepares it for input into the TensorFlow style transfer model
def load_image(image_array, target_size=None):
    image = Image.fromarray(image_array)
    if target_size:
        image = image.resize(target_size)
    image = np.array(image) / 255.0
    image = np.expand_dims(image, axis=0)
    return tf.convert_to_tensor(image, dtype=tf.float32)

# Applies style transfer using the Magenta model
def apply_style_transfer(content_array, style_array, output_path):
    content_image = load_image(content_array)
    style_image = load_image(style_array)
    stylized = hub_model(tf.constant(content_image), tf.constant(style_image))[0]
    save_image(stylized, output_path)

# Saves a TensorFlow tensor as an image file
def save_image(image_tensor, filename):
    image = image_tensor[0].numpy()
    image = (image * 255).astype(np.uint8)
    Image.fromarray(image).save(filename)

# Computes optical flow between two images using the RAFT model
def compute_flow(raft_model, img1, img2):
    image1 = torch.from_numpy(img1).permute(2, 0, 1).float()[None].cuda()
    image2 = torch.from_numpy(img2).permute(2, 0, 1).float()[None].cuda()
    padder = InputPadder(image1.shape)
    image1, image2 = padder.pad(image1, image2)

    with torch.amp.autocast("cuda", enabled=False):
        _, flow_up = raft_model(image1, image2, iters=12, test_mode=True)
    return flow_up[0].permute(1, 2, 0).detach().cpu().numpy()

# Warps an image based on an optical flow field
def warp_frame_with_flow(image, flow):
    h, w = flow.shape[:2]
    flow_map = np.meshgrid(np.arange(w), np.arange(h))
    flow_map = np.stack(flow_map, axis=-1).astype(np.float32)
    flow_map += flow
    warped = cv2.remap(image, flow_map, None, cv2.INTER_LINEAR)
    return warped

# Orchestrates the full video processing pipeline:
# extracts audio, frames, applies style, and recombines output
def process_video(raw_video_path, style_video_path, output_path):
    raw_dir = "temp/raw_frames"
    styled_dir = "temp/styled_frames"
    os.makedirs(raw_dir, exist_ok=True)
    os.makedirs(styled_dir, exist_ok=True)

    audio_path = "temp/audio.aac"
    extract_audio(raw_video_path, audio_path)

    # Optional: stabilize first
    stabilized_video = "temp/stabilized_input.mp4"
    stabilize_video(raw_video_path, stabilized_video)
    num_frames, fps = video_to_frames(stabilized_video, raw_dir)

    raft_model = load_raft_model(raft_weights_path)

    # Composite style frame
    style_indices = [10, 50, 100]
    style_frames = []
    cap = cv2.VideoCapture(style_video_path)
    for idx in style_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            frame = denoise_frame(frame)  # Denoise style frames too
            style_frames.append(frame.astype(np.float32) / 255.0)
    cap.release()

    composite_style = np.zeros_like(style_frames[0])
    weights = [0.4, 0.3, 0.3]
    for sf, w in zip(style_frames, weights):
        composite_style += sf * w
    composite_style = np.clip(composite_style, 0, 1)
    style_frame = (composite_style * 255).astype(np.uint8)

    prev_img = None
    prev_styled = None

    for i in range(num_frames):
        raw_path = os.path.join(raw_dir, f"frame_{i:04d}.png")
        out_path = os.path.join(styled_dir, f"styled_frame_{i:04d}.png")
        curr_img = cv2.imread(raw_path)

        if prev_img is not None and prev_styled is not None:
            flow = compute_flow(raft_model, prev_img, curr_img)
            warped = warp_frame_with_flow(prev_styled, flow)
            blend_input = cv2.addWeighted(curr_img, 0.7, warped, 0.3, 0)  # smoother blend
        else:
            blend_input = curr_img

        apply_style_transfer(blend_input, style_frame, out_path)

        prev_img = curr_img
        prev_styled = cv2.imread(out_path)

    frames_to_video(styled_dir, output_path, fps, audio_path)

    shutil.rmtree("temp")
    print("Done! Final video saved to:", output_path)

# Test
raw_video_path = "path/to/your/raw_video.mp4"
style_video_path = "path/to/your/style_video.mov"
output_video_path = "styled_output.mp4"
raft_weights_path = "/content/RAFT/models/raft-things.pth"
process_video(raw_video_path, style_video_path, output_video_path)

"""#METRICS

#SSIM
"""

from skimage.metrics import structural_similarity as ssim
import cv2
import moviepy.editor as mp

def calculate_average_ssim(video_path1, video_path2):
    """Calculate the average SSIM between frames of two videos."""
    clip1 = mp.VideoFileClip(video_path1)
    clip2 = mp.VideoFileClip(video_path2)

   # Compare only up to the number of frames in the shorter video to avoid indexing errors
    num_frames = min(len(list(clip1.iter_frames())), len(list(clip2.iter_frames())))
    total_ssim = 0.0

    for i, (frame1, frame2) in enumerate(zip(clip1.iter_frames(), clip2.iter_frames())):
        if i >= num_frames:
            break

        # Resize frames to have the same dimensions
        frame1 = cv2.resize(frame1, (frame2.shape[1], frame2.shape[0]))  # Resize frame1 to match frame2's size

        # Convert frames to grayscale for SSIM comparison
        gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY)
        gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY)

        # Compute SSIM
        ssim_value = ssim(gray_frame1, gray_frame2)
        total_ssim += ssim_value

    # Average SSIM
    avg_ssim = total_ssim / num_frames
    return avg_ssim

# TEST
pre_style_ssim = calculate_average_ssim(raw_video_path, style_video_path)
post_style_ssim = calculate_average_ssim(output_video_path, style_video_path)

print(f"Average SSIM before style transfer: {pre_style_ssim}")
print(f"Average SSIM after style transfer: {post_style_ssim}")

"""#TWE"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

def compute_TWE(video_path):
    # Open the video
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Failed to open video.")
        return None

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    print(f"Video opened successfully. Number of frames: {frame_count}, FPS: {fps}")

    if frame_count < 2:
        print("Not enough frames to compute TWE.")
        cap.release()
        return None

    prev_gray = None
    warp_errors = []

    # Initialize TV-L1 Optical Flow
    optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()

    frame_idx = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if prev_gray is not None:
            # Compute Optical Flow between frames
            flow = optical_flow.calc(prev_gray, gray, None)

            # Warp previous frame toward current frame
            h, w = prev_gray.shape
            flow_map = np.stack(np.meshgrid(np.arange(w), np.arange(h)), axis=-1).astype(np.float32)
            flow_map += flow
            warped_prev = cv2.remap(prev_gray, flow_map, None, cv2.INTER_LINEAR)

            # Calculate error
            error = np.abs(warped_prev.astype(np.float32) - gray.astype(np.float32))
            mean_error = np.mean(error) / 255.0  # normalize between 0 and 1
            warp_errors.append(mean_error)

        prev_gray = gray
        frame_idx += 1

    cap.release()

    if not warp_errors:
        print("No valid frame pairs to compute TWE.")
        return None

    twe = np.mean(warp_errors)
    print(f"Final Temporal Warping Error (TWE): {twe:.6f}")
    return twe

#Test
video_path = "styled_output.mp4"
compute_TWE(video_path)